import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
%matplotlib inline

df = pd.read_csv(r'd:\dataset\credit\CreditScoring.csv')
df.columns = df.columns.str.lower()
#df.head()

#conversão dos atributos que estão numéricos mas deveriam ser categóricos
status_values = {
    1: 'ok',
    2: 'default',
    0: 'unk' #valor desconhecido
}
df.status = df.status.map(status_values)

home_values = {
    1: 'rent',
    2: 'owner',
    3: 'private',
    4: 'ignore',
    5: 'parents',
    6: 'other',
    0: 'unk'
}
df.home = df.home.map(home_values)

marital_values = {
    1: 'single',
    2: 'married',
    3: 'window',
    4: 'separated',
    5: 'divorced',
    0: 'unk'
}
df.marital = df.marital.map(marital_values)

record_values = {
    1: 'no',
    2: 'yes',
    0: 'unk'
}
df.records = df.records.map(records_values)


job_values = {
    1: 'fixed',
    2: 'parttime',
    3: 'freelance',
    4: 'others',
    0: 'unk'
}
df.job = df.job.map(job_values)

#mostrar estatisticas das colunas
df.describe().round()

#Converter os valores 99999999 para NAN
for c in ['income', 'assets','debt']:
    df[c]=df[c].replace(to_replace=99999999, value = np.nan)
df.head()
df.describe().round()

#analisar a variavel alvo (status)
df.status.value_counts()
#remover a única linha 'unk'
df = df[df.status != 'unk' ]
df.status.value_counts()

#divisão do dataset (60% treinamento, 20% validação, 20% teste)
from sklearn.model_selection import train_test_split
df_train_full , df_test = train_test_split(df, test_size=0.2, random_state=11)
df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=11)
#para ver o tamanho dos datasets
len(df_train), len(df_val), len(df_test)

#Obter os valores da variável alvo de treinamento
# y=1 -> não pagou, y=0 -> pagou
y_train = (df_train.status=='default').values
y_val = (df_val.status == 'default').values

#remover a variável alvo do dataset
del df_train['status']
del df_val['status']

#substituir NAN por zero 
df_train = df_train.fillna(0)
df_val = df_val.fillna(0)
df.describe().round()

#Usar on-hot encoding para codificar as variáveis categóricas
# Se valor presente = 1, se ausente = 0

#Converter DicVectorizer para um dicionario 
dict_train = df_train.to_dict(orient='records')
dict_val = df_val.to_dict(orient='records')
from sklearn.feature_extraction import DictVectorizer
dv = DictVectorizer(sparse=False)
#Usar on-hot encoding
X_train = dv.fit_transform(dict_train)
X_val = dv.transform(dict_val)

#-------------------  Floresta randômica 
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

#verificar quantas arvores são necessarias
aucs=[]
for i in range(10, 201, 10): 
    rf = RandomForestClassifier(n_estimators=i, random_state=3)
    rf.fit(X_train, y_train)
    y_pred = rf.predict_proba(X_val)[:,1]
    auc = roc_auc_score(y_val, y_pred)
    print('%s -> %.3f' % (i, auc))
    aucs.append(auc)
#plt.plot(range(10, 201, 10), aucs)  

#verificar melhor profundidade

all_aucs={}
for depth in [5, 10, 20]: #níveis de profundidade
    print('depth: %s' %depth )
    aucs = []
    for i in range(10, 201, 10): 
        rf = RandomForestClassifier(n_estimators=i, max_depth=depth, random_state=1)
        rf.fit(X_train, y_train)
        y_pred = rf.predict_proba(X_val)[:,1]
        auc = roc_auc_score(y_val, y_pred)
        print('%s->%.3f' %(i,auc))
        aucs.append(auc)
    all_aucs[depth] = aucs
    print()
#num_trees = list(range(10,201,10))
#plt.plot(num_trees, all_aucs[5], label = 'depth=5')
#plt.plot(num_trees, all_aucs[10], label = 'depth=10')
#plt.plot(num_trees, all_aucs[20], label = 'depth=20')
#plt.legend() #melhor depth = 10

#verificar melhor min_sample_leaf
all_aucs={}
for m in[3,5,10]:
    aucs=[]
    for i in range(10, 201, 20):
        rf = RandomForestClassifier(n_estimators=i, max_depth=10, 
                                       min_samples_leaf=m, random_state=1)
        rf.fit(X_train, y_train)
        y_pred = rf.predict_proba(X_val)[:,1]
        auc = roc_auc_score(y_val, y_pred)
        print('%s->%.3f' %(i,auc))
        aucs.append(auc)
    all_aucs[m]= aucs
    print()
num_trees = list(range(10, 201, 20))
plt.plot(num_trees, all_aucs[3], label = 'min_samples_leaf=3')
plt.plot(num_trees, all_aucs[5], label = 'min_samples_leaf=5')
plt.plot(num_trees, all_aucs[10], label = 'min_samples_leaf=10')
plt.legend() #melhor min_salmple_leaf = 5

#--------Modelo Final -----------
rf = RandomForestClassifier(n_estimators = 200, max_depth=10, 
    min_samples_leaf=5, random_state=1)
